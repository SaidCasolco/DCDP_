{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/02%20An%C3%A1lisis%20Estad%C3%ADstico/notebooks/4_WebScrapping.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contenido\"></a>\n",
    "<h1><center>Contenido | Módulo 2</center><h1>\n",
    "    \n",
    "---\n",
    "* [Introducción al Web Scrapping con Python](#a)   \n",
    "* [Breve ejemplo](#b) \n",
    "* [CoronaScrapp](#c) \n",
    "* [Referencias](#d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a\"></a>\n",
    "<h1><center>2.11. Introducción - Web Scrapping</center></h1>\n",
    "\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n",
    "![alt text](https://www.grid.cl/blog/wp-content/uploads/2019/03/001-efficient-web-scraping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que **INTERNET** está compuesta por muchos millones de documentos enlazados entre sí, conocidos también como páginas web. \n",
    "\n",
    "El texto fuente de las páginas web está escrito en lenguaje Hypertext Markup Language (HTML). Los códigos fuente en HTML son una mezcla de informaciones legibles para los humanos y códigos legibles para las máquinas, llamados tags o etiquetas. El navegador, como puede ser Chrome, Firefox, Safari o Edge, procesa el texto fuente, interpreta las etiquetas y presenta al usuario la información que contienen.\n",
    "\n",
    "Para extraer del texto fuente únicamente la información que le interesa al usuario, se utiliza un <font color=red>software especial</font>. Se trata de los programas llamados web scrapers, crawlers, spiders o, simplemente, bots, que examinan el texto fuente de las páginas en busca de patrones concretos y extraen la información que contienen. Los datos conseguidos mediante web scraping posteriormente se resumen, combinan, evalúan o almacenan para ser usados más adelante.\n",
    "\n",
    "En esta notebook veremos un poco del por qué Python resulta especialmente útil para la creación de web scrapers y una introducción a este tema junto con unos ejemplos ([también se puede hacer en R](https://www.r-bloggers.com/2019/07/beautifulsoup-vs-rvest/)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraping en términos generales\n",
    "\n",
    "El esquema básico del web scraping es sencillo de explicar.... \n",
    "\n",
    "En primer lugar, el desarrollador del scraper analiza el texto fuente en HTML de la página web en cuestión. Por lo general, encontrará patrones claros que permitirán extraer la información deseada. El scraper será entonces programado para identificar dichos patrones y realizará el resto del trabajo automáticamente:\n",
    "\n",
    "   * Abrir la página web a través del URL\n",
    "   * Extraer automáticamente los datos estructurados a partir de los patrones\n",
    "   * Resumir, almacenar, evaluar o combinar los datos extraídos, entre otras acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de aplicación del web scraping\n",
    "\n",
    "El web scraping puede tener aplicaciones muy diversas. Además de la indexación de buscadores, el web scraping también puede usarse con los siguientes fines, entre muchos otros:\n",
    "\n",
    "  * Crear bases de datos de contactos\n",
    "  * Controlar y comparar ofertas online\n",
    "  * Reunir datos de diversas fuentes online\n",
    "  * Observar la evolución de la presencia y la reputación online\n",
    "  * Reunir datos financieros, meteorológicos o de otro tipo\n",
    "  * Observar cambios en el contenido de páginas web\n",
    "  * Reunir datos con fines de investigación\n",
    "  * Realizar exploraciones de datos o data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herramientas de scraping para Python\n",
    "\n",
    "Python incluye diversas herramientas consolidadas para realizar proyectos de scraping:\n",
    "\n",
    "   * [Scrapy](https://scrapy.org/)\n",
    "   * [Selenium](https://selenium-python.readthedocs.io/)\n",
    "   * [BeautifulSoup](https://pypi.org/project/beautifulsoup4/)\n",
    "\n",
    "A continuación, nos enfocaremos solamente BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura de la página HTML\n",
    "\n",
    "El lenguaje de marcado de hipertexto (HTML) es el lenguaje de marcado estándar para documentos diseñados para mostrarse en un navegador web. HTML describe la estructura de una página web y se puede utilizar con hojas de estilo en cascada (CSS) y un lenguaje de secuencias de comandos como JavaScript para crear sitios web interactivos. HTML consta de una serie de elementos que \"le dicen\" al navegador cómo mostrar el contenido. Por último, los elementos se representan mediante etiquetas.\n",
    "\n",
    "Aquí hay algunas etiquetas:\n",
    "\n",
    "    La declaración <!DOCTYPE html> define este documento como HTML5.\n",
    "    El elemento <html> es el elemento raíz de una página HTML.\n",
    "    La etiqueta <div> define una división o una sección en un documento HTML. Suele ser un contenedor de otros elementos.\n",
    "    El elemento <head> contiene metainformación sobre el documento.\n",
    "    El elemento <title> especifica un título para el documento.\n",
    "    El elemento <body> contiene el contenido de la página visible.\n",
    "    El elemento <h1> define un encabezado grande.\n",
    "    El elemento <p> define un párrafo.\n",
    "    El elemento <a> define un hipervínculo.\n",
    "\n",
    "Las etiquetas HTML normalmente vienen en pares como $<p>$ y $</p>$. La primera etiqueta de un par es la etiqueta de apertura, la segunda etiqueta es la etiqueta de cierre. La etiqueta final se escribe como la etiqueta inicial, pero con una barra diagonal insertada antes del nombre de la etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tiene una estructura en forma de árbol 🌳 🌲 gracias al Modelo de objetos de documento (DOM), una interfaz multiplataforma e independiente del idioma. Así es como se ve un árbol HTML muy simple. \n",
    "![img](https://mechomotive.com/wp-content/uploads/2021/07/HTML-document-tree-representation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CALIPSO\\AppData\\Local\\Temp\\ipykernel_8408\\2058709175.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\" dir=\"ltr\">\n",
       "<head>\n",
       "  <title>Intro to HTML</title>\n",
       "</head>\n",
       "\n",
       "<body>\n",
       "  <h1>Heading h1</h1>\n",
       "  <h2>Heading h2</h2>\n",
       "  <h3>Heading h3</h3>\n",
       "  <h4>Heading h4</h4>\n",
       "\n",
       "  <p>\n",
       "    That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
       "    You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
       "  </p>\n",
       "\n",
       "  <p>\n",
       "    This <br> is a paragraph <br> with <br> line breaks\n",
       "  </p>\n",
       "\n",
       "  <p style=\"color:red\">\n",
       "    Add colour to your paragraphs.\n",
       "  </p>\n",
       "\n",
       "  <p>Unordered list:</p>\n",
       "  <ul>\n",
       "    <li>Python</li>\n",
       "    <li>R</li>\n",
       "    <li>Julia</li>\n",
       "  </ul>\n",
       "\n",
       "  <p>Ordered list:</p>\n",
       "  <ol>\n",
       "    <li>Data collection</li>\n",
       "    <li>Exploratory data analysis</li>\n",
       "    <li>Data analysis</li>\n",
       "    <li>Policy recommendations</li>\n",
       "  </ol>\n",
       "  <hr>\n",
       "\n",
       "  <!-- This is a comment -->\n",
       "\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\" dir=\"ltr\">\n",
    "<head>\n",
    "  <title>Intro to HTML</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "  <h1>Heading h1</h1>\n",
    "  <h2>Heading h2</h2>\n",
    "  <h3>Heading h3</h3>\n",
    "  <h4>Heading h4</h4>\n",
    "\n",
    "  <p>\n",
    "    That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
    "    You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    This <br> is a paragraph <br> with <br> line breaks\n",
    "  </p>\n",
    "\n",
    "  <p style=\"color:red\">\n",
    "    Add colour to your paragraphs.\n",
    "  </p>\n",
    "\n",
    "  <p>Unordered list:</p>\n",
    "  <ul>\n",
    "    <li>Python</li>\n",
    "    <li>R</li>\n",
    "    <li>Julia</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>Ordered list:</p>\n",
    "  <ol>\n",
    "    <li>Data collection</li>\n",
    "    <li>Exploratory data analysis</li>\n",
    "    <li>Data analysis</li>\n",
    "    <li>Policy recommendations</li>\n",
    "  </ol>\n",
    "  <hr>\n",
    "\n",
    "  <!-- This is a comment -->\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herramientas para desarrolladores de Chrome\n",
    "\n",
    "[Chrome DevTools](https://developer.chrome.com/docs/devtools/) es un conjunto de herramientas para desarrolladores web integradas directamente en el navegador Google Chrome. DevTools puede ayudar a ver y editar páginas web. Usaremos la herramienta de Chrome para inspeccionar una página HTML y encontrar qué elementos corresponden a los datos que podríamos querer raspar.\n",
    "ejercicio corto\n",
    "\n",
    "Para obtener algo de experiencia con la estructura de la página HTML y Chrome DevTools, buscaremos y ubicaremos elementos en IMDB.\n",
    "\n",
    "Sugerencia: Pulse Comando+Opción+C (Mac) o Control+Mayús+C (Windows, Linux) para acceder al panel de elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping con `requests` y `BeautifulSoup`\n",
    "\n",
    "Usaremos `requests` y `BeautifulSoup` para acceder y raspar el contenido de [la página de inicio de IMDB](https://www.imdb.com).\n",
    "\n",
    "### ¿Qué es `BeautifulSoup`?\n",
    "\n",
    "Es una biblioteca de Python para extraer datos de archivos HTML y XML. Proporciona métodos para navegar por la estructura de árbol del documento que discutimos antes y raspar su contenido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests import get\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>403 Forbidden</title>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB's homepage\n",
    "imdb_url = 'https://www.imdb.com'\n",
    "\n",
    "# Usamos requests para obtener los datos de la URL dada\n",
    "imdb_response = requests.get(imdb_url)\n",
    "\n",
    "# Transformamos todo el codigo HTML usando beautiful soup\n",
    "imdb_soup = BeautifulSoup(imdb_response.text, 'html.parser')\n",
    "\n",
    "# Titulo de la pagina transformada\n",
    "imdb_soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'403 Forbidden'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos obtenerlo de igual forma sin los tags de HTML\n",
    "imdb_soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.imdb.com/chart/top'\n",
    "page=get(url).content\n",
    "soup=BeautifulSoup(page,'html.parser')\n",
    "class_=soup.find_all(name='div',attrs={'class':'wlb_ribbon'})\n",
    "movie_ids=[c['data-tconst'] for c in class_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://www.omdbapi.com/?i=\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#print(url+movie_ids[i]+\"&apikey=de12b217\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m r\u001b[38;5;241m=\u001b[39mrequests\u001b[38;5;241m.\u001b[39mget(url\u001b[38;5;241m+\u001b[39mmovie_ids[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&apikey=de12b217\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      8\u001b[0m     movie_info[i]\u001b[38;5;241m.\u001b[39mappend(r[a])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "movie_info=[[] for i in range(len(movie_ids))]\n",
    "\n",
    "for i in range(250):\n",
    "    url='http://www.omdbapi.com/?i='\n",
    "    #print(url+movie_ids[i]+\"&apikey=de12b217\")\n",
    "    r=requests.get(url+movie_ids[i]+\"&apikey=de12b217\").json()\n",
    "    for a in r.keys():\n",
    "        movie_info[i].append(r[a])\n",
    "        \n",
    "df_omdb=pd.DataFrame(movie_info,columns=r.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.imdb.com/title/'\n",
    "t='/plotsummary?ref_=tt_stry_pl'\n",
    "plot=[[] for i in range(len(movie_ids))]\n",
    "for i in range(250):\n",
    "    #print(url+df_omdb.imdbID[i]+t)\n",
    "    page=get(url+df_omdb.imdbID[i]+t).content\n",
    "    soup=BeautifulSoup(page,'html.parser')\n",
    "    class_=soup.find_all(name='li',attrs={'class':'ipl-zebra-list__item'})\n",
    "    for j in class_:\n",
    "        plot[i].append(j.get_text(strip = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb['Plot']=plot\n",
    "df_omdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "\n",
    "El primer paso para limpiar los datos es convertir Year en una variable categórica. \n",
    "\n",
    "Se elegirá el año del 2000 como corte adecuado. Las películas lanzadas antes del 2000 se convirtieron en 0 y después de 2000 en 1. Después de hacer esto, realizamos un one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.Year=pd.to_numeric(df_omdb.Year)\n",
    "for i in range(250):\n",
    "    if df_omdb.Year[i]<2000:\n",
    "        df_omdb.Year[i]=0\n",
    "    else:\n",
    "        df_omdb.Year[i]=1\n",
    "dummy_year=pd.get_dummies(df_omdb.Year)\n",
    "\n",
    "for i in range(250):\n",
    "    df_omdb.Runtime[i]=df_omdb.Runtime[i].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb['Runtime'] = pd.to_numeric(df_omdb['Runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(10,5))\n",
    "ax.hist(df_omdb['Runtime'],edgecolor='white',align='right')\n",
    "ax.axvline(x=np.mean(df_omdb['Runtime']),c='r')\n",
    "ax.axvline(x=np.mean(df_omdb['Runtime'])-np.std(df_omdb['Runtime']),c='b',ls='--')\n",
    "ax.axvline(x=np.mean(df_omdb['Runtime'])+np.std(df_omdb['Runtime']),c='b',ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb['Runtime']=pd.to_numeric(df_omdb['Runtime'],errors='coerce')\n",
    "for i in range(250):\n",
    "    if df_omdb.Runtime[i]<=125:\n",
    "        df_omdb.Runtime[i]=0\n",
    "    else: \n",
    "        df_omdb.Runtime[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(column_name):\n",
    "    \"\"\"This function takes a column from the dataframe and splits two elements\n",
    "       if they are separated by a comma.\n",
    "       For ex. in Actors column there might be values such as Christian Bale, Morgan Freeman.\n",
    "       This will separate these two actors and store them individually in a list.\"\"\"\n",
    "    name=set()\n",
    "    for name_string in df_omdb[column_name]:\n",
    "        name.update(name_string.split(', '))\n",
    "    name=sorted(name)\n",
    "    return name\n",
    "\n",
    "def top(column_name):\n",
    "    \"\"\"This function takes its input as name of the column and returns a sorted list of the \n",
    "       elements which occur very frequently in that column in descending order.\"\"\"\n",
    "    \n",
    "    name=clean(column_name)\n",
    "    dummy_name=pd.DataFrame()\n",
    "    for n in name:\n",
    "        dummy_name[n]=[int(n in nm.split(', ')) for nm in df_omdb[column_name]] \n",
    "    \n",
    "    namelist=[n for n in name]\n",
    "    nlt=dummy_name[namelist].sum()\n",
    "    nlt=nlt.sort_values(axis=0,ascending=False)\n",
    "    return nlt.index\n",
    "    \n",
    "def plot_column(column_name,n_elem_display=0):\n",
    "    \"\"\" This function is used to plot a bar graph of a column of the dataframe.\n",
    "        It takes its argument as name of column and number of elements to display and\n",
    "        return a bar graph of the user defined number of top elements which occur\n",
    "        frequently in that column.\"\"\"\n",
    "    \n",
    "    name=clean(column_name)\n",
    "    dummy_name=pd.DataFrame()\n",
    "    for n in name:\n",
    "        dummy_name[n]=[int(n in nm.split(', ')) for nm in df_omdb[column_name]] \n",
    "    \n",
    "    namelist=[n for n in name]\n",
    "    nlt=dummy_name[namelist].sum()\n",
    "    nlt=nlt.sort_values(axis=0,ascending=False)\n",
    "    if n_elem_display !=0:\n",
    "        return nlt[:n_elem_display].plot(kind = \"bar\",figsize=(10,10))\n",
    "    else:\n",
    "        return nlt[:].plot(kind = \"bar\",figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegiremos todos los géneros como nuestros predictores en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique genres contained in the dataframe\n",
    "genres=clean('Genre')\n",
    "#Add one column for every genre in the dataframe\n",
    "for genre in genres:\n",
    "    df_omdb[\"genre:\"+genre] = [int(genre in g.split(', ')) for g in df_omdb.Genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analicemos la cantidad de actores que se pueden usar como predictores en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Actors',30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, podemos tomar a los 30 actores principales, cada uno con más de 3 películas, en la lista de 250 películas principales de imdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding actors to our dataset\n",
    "actors=top('Actors')\n",
    "actors\n",
    "for actor in actors[:30]:\n",
    "    df_omdb[\"Actor:\"+actor] = [int(actor in a.split(', ')) for a in df_omdb.Actors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ \n",
    "\n",
    "Ahora los Directores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Director',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directors=top('Director')\n",
    "    \n",
    "for director in directors[:20]:\n",
    "    df_omdb[\"Director:\"+director] = [int(director in d.split(', ')) for d in df_omdb.Director]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar si tomar escritores o no como predictores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers1=set()\n",
    "writers2=set()\n",
    "for writer_string in df_omdb.Writer:\n",
    "    writers1.update(writer_string.split(', '))\n",
    "for j in writers1:\n",
    "    writers2.update(j.rsplit(' (')[:1])\n",
    "writers2 = sorted(writers2)\n",
    "\n",
    "dummy_writers=pd.DataFrame()\n",
    "\n",
    "# Add one column for every writer in the dataframe\n",
    "for writer in writers2:\n",
    "    dummy_writers[writer] = [int(writer in w.split(', ')) for w in df_omdb.Writer]   \n",
    "dummy_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writerlist=[w for w in writers2]\n",
    "wlt=dummy_writers[writerlist].sum()\n",
    "wlt=wlt.sort_values(axis=0,ascending=False)\n",
    "wlt.iloc[0:10].plot(kind = \"bar\",figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que no hay muchos escritores que tengan un número significativo de películas, decidimos no tomar a los escritores como uno de nuestros predictores.\n",
    "\n",
    "Ahora, exploraremos el predictor de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Language',11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column('Country',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding all of the top 10 countries to our datset\n",
    "countries=top('Country')\n",
    "\n",
    "for country in countries[:10]:\n",
    "    df_omdb[\"Country:\"+country] = [int(country in c.split(', ')) for c in df_omdb.Country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "sns.countplot(df_omdb['Metascore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc(data,bgcolor,title):\n",
    "    plt.figure(figsize = (100,100))\n",
    "    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n",
    "    wc.generate(' '.join(data))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc(df_omdb,'black','Most Used Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Web scraping con Scrapy\n",
    "\n",
    "Scrapy, una de las herramientas para hacer web scraping con Python que presentamos, utiliza un analizador sintáctico o parser HTML para extraer datos del texto fuente (en HTML) de la web siguiendo este esquema:\n",
    "\n",
    "$$URL → Solicitud HTTP → HTML → Scrapy$$\n",
    "\n",
    "El concepto clave del desarrollo de scrapers con Scrapy son los llamados web spiders, programas de scraping sencillos y basados en Scrapy. Cada spider (araña) está programado para scrapear una web concreta y va descolgándose de página a página. La programación usada está orientada a objetos: cada spider es una clase de Python propia.\n",
    "\n",
    "Además del paquete de Python en sí, la instalación de Scrapy incluye una herramienta de línea de comandos, la Scrapy Shell, que permite controlar los spiders. Además, los spiders ya creados pueden almacenarse en la Scrapy Cloud. Desde allí, se ejecutan con tiempos establecidos. De esta forma pueden scrapearse también sitios web complejos sin necesidad de utilizar para ello el propio ordenador ni la propia conexión a Internet. Otra manera de hacerlo es crear un servidor de web scraping propio usando el software de código abierto Scrapyd.\n",
    "\n",
    "Scrapy es una plataforma consolidada para aplicar técnicas de web scraping con Python. Su arquitectura está orientada a las necesidades de proyectos profesionales. Scrapy cuenta, por ejemplo, con un pipeline integrado para procesar los datos extraídos. La apertura de las páginas en Scrapy se produce de forma asíncrona, es decir, con la posibilidad de descargar varias páginas simultáneamente. Por ello, Scrapy es una buena opción para proyectos de scraping que hayan de procesar de grandes volúmenes de páginas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Web scraping con Selenium\n",
    "\n",
    "El software libre Selenium es un framework para realizar test automatizados de software a aplicaciones web. En principio, fue desarrollado para poner a prueba páginas y apps web, pero el WebDriver de Selenium también puede usarse con Python para realizar scraping. Si bien Selenium en sí no está escrito en Python, con este lenguaje de programación es posible acceder a las funciones del software.\n",
    "\n",
    "A diferencia de Scrapy y de BeautifulSoup, Selenium no trabaja con el texto fuente en HTML de la web en cuestión, sino que carga la página en un navegador sin interfaz de usuario. El navegador interpreta entonces el código fuente de la página y crea, a partir de él, un Document Object Model (modelo de objetos de documento o DOM). Esta interfaz estandarizada permite poner a prueba las interacciones de los usuarios. De esta forma se consigue, por ejemplo, simular clics y rellenar formularios automáticamente. Los cambios en la web que resultan de dichas acciones se reflejan en el DOM. La estructura del proceso de web scraping con Selenium es la siguiente:\n",
    "\n",
    "$$URL → Solicitud HTTP → HTML → Selenium → DOM$$\n",
    "\n",
    "Puesto que el DOM se genera de manera dinámica, Selenium permite scrapear también páginas cuyo contenido ha sido generado mediante JavaScript. El acceso a contenidos dinámicos es la ventaja más importante de Selenium. En términos prácticos, Selenium también puede usarse en combinación con Scrapy o con BeautifulSoup: Selenium proporcionaría el texto fuente, mientras que la segunda herramienta se encargaría del análisis sintáctico y la evaluación de los datos. En este caso, el esquema que se seguiría tendría esta forma:\n",
    "\n",
    "$$URL → Solicitud HTTP → HTML → Selenium → DOM → HTML → Scrapy / BeautifulSoup$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Web scraping con BeautifulSoup\n",
    "\n",
    "De las tres herramientas que presentamos para realizar web scraping con Python, BeautifulSoup es la más antigua. Al igual que en el caso de Scrapy, se trata de un parser o analizador sintáctico HTML. El web scraping con BeautifulSoup tiene la siguiente estructura:\n",
    "\n",
    "$$URL → Solicitud HTTP → HTML → BeautifulSoup$$\n",
    "\n",
    "Sin embargo, a diferencia de Scrapy, en BeautifulSoup el desarrollo del scraper no requiere una programación orientada a objetos, sino que el scraper se redacta como una sencilla secuencia de comandos o script. Con ello, BeautifulSoup ofrece el método más fácil para pescar información de la sopa de tags a la que hace honor su nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### En resumen\n",
    "\n",
    "¿Qué herramienta deberías elegir para tu proyecto? \n",
    "\n",
    "En resumen: escoge **BeautifulSoup** si necesitas un desarrollo rápido o si quieres familiarizarte primero con los conceptos de Python y de web scraping. **Scrap**y, por su parte, te permite realizar complejas aplicaciones de web scraping en Python si dispones de los conocimientos necesarios. **Selenium** será tu mejor opción si tu prioridad es extraer contenidos dinámicos con Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"b\"></a>\n",
    "### 2.11.1 Breves ejemplos de web scraping\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n",
    "\n",
    " * Extraer citas y autores con Python y BeautifulSoup\n",
    "\n",
    "La página [web Quotes to Scrape](http://quotes.toscrape.com/) ofrece toda una colección de citas de personajes famosos pensadas especialmente para ser objeto de test de scraping, para que no tengas que preocuparte por incumplir las condiciones de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar módulos\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "# Dirección de la página web\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "# Ejecutar GET-Request\n",
    "response = requests.get(url)\n",
    "# Analizar sintácticamente el archivo HTML de BeautifulSoup del texto fuente\n",
    "html = BeautifulSoup(response.text, 'html.parser')\n",
    "# Extraer todas las citas y autores del archivo HTML\n",
    "quotes_html = html.find_all('span', class_=\"text\")\n",
    "authors_html = html.find_all('small', class_=\"author\")\n",
    "# Crear una lista de las citas\n",
    "quotes = list()\n",
    "for quote in quotes_html:\n",
    "    quotes.append(quote.text)\n",
    "# Crear una lista de los autores\n",
    "authors = list()\n",
    "for author in authors_html:\n",
    "    authors.append(author.text) \n",
    "# Para hacer el test: combinar y mostrar las entradas de ambas listas\n",
    "for t in zip(quotes, authors):\n",
    "    print(t)\n",
    "# Guardar las citas y los autores en un archivo CSV en el directorio actual\n",
    "# Abrir el archivo con Excel / LibreOffice, etc.\n",
    "with open('./zitate.csv', 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, dialect='excel')\n",
    "    csv_writer.writerows(zip(quotes, authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"c\"></a>\n",
    "### 2.11.2 Scrapping Coronavirus\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "from tabulate import tabulate \n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today=datetime.date.today().strftime(\"%m-%d-%Y\")\n",
    "data_date=datetime.date.today()-datetime.timedelta(days=1)\n",
    "print(\"Today is {}\".format(today))\n",
    "data_date=data_date.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.worldometers.info/coronavirus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get web data\n",
    "req = requests.get(url)\n",
    "response = req.content\n",
    "# parse web data\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the table\n",
    "#table is in the last of the page\n",
    "\n",
    "thead= soup.find_all('thead')[-1]\n",
    "print(thead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all rows in thead\n",
    "head = thead.find_all('tr')\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the table data content\n",
    "tbody = soup.find_all('tbody')[0]\n",
    "tbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = tbody.find_all('tr')\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the table contents\n",
    "\n",
    "# container for  column title\n",
    "head_rows = []\n",
    "\n",
    "\n",
    "# loop through the head and append each row to head\n",
    "for tr in head:\n",
    "    td = tr.find_all(['th', 'td'])\n",
    "    row = [i.text for i in td]\n",
    "    head_rows.append(row)\n",
    "print(head_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container for contents\n",
    "body_rows = []\n",
    "\n",
    "# loop through the body and append each row to body\n",
    "for tr in body:\n",
    "    td = tr.find_all(['th', 'td'])\n",
    "    row = [i.text for i in td]\n",
    "    body_rows.append(row)\n",
    "print(body_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bs = pd.DataFrame(body_rows[:len(body_rows)],columns=head_rows[0]) \n",
    "df_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continentdata\n",
    "cols=['Continent','TotalCases', 'NewCases', 'TotalDeaths', 'NewDeaths', 'TotalRecovered',\n",
    "       'NewRecovered', 'ActiveCases', 'Serious,Critical', ]\n",
    "\n",
    "continent_data = df_bs.iloc[:8, :-3].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# drop unwanted columns\n",
    "continent_data = continent_data.drop('#', axis=1)\n",
    "#rearrange Columns Sequence\n",
    "continent_data = continent_data[cols]\n",
    "continent_data['Continent'].loc[6]=\"Not Assigned\"\n",
    "continent_data['Continent'].loc[7]=\"World\"\n",
    "\n",
    "\n",
    "continent_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "<a id=\"d\"></a>\n",
    "<h1><center>Referencias y links de interés</center></h1>\n",
    "\n",
    "[Regreso a contenido](#contenido)\n",
    "\n",
    "---\n",
    "\n",
    "* [Rvest para R](https://www.r-bloggers.com/2019/07/beautifulsoup-vs-rvest/)\n",
    "* [Scrapy](https://scrapy.org/)\n",
    "* [Selenium](https://selenium-python.readthedocs.io/)\n",
    "* [BeautifulSoup](https://pypi.org/project/beautifulsoup4/)\n",
    "\n",
    "-------\n",
    "\n",
    "* [Tutorial Scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n",
    "* [Ejemplos de Scrapy](https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/)\n",
    "\n",
    "-------\n",
    "* [Tutorial Selenium](https://selenium-python.readthedocs.io/getting-started.html)\n",
    "* [Ejemplos de Selenium](https://www.guru99.com/selenium-python.html)\n",
    "\n",
    "-------\n",
    "\n",
    "* [Tutorial de BeautifulSoap](https://www.dataquest.io/blog/web-scraping-tutorial-python/)\n",
    "* [Ejemplos de BeautifulSoap](https://realpython.com/beautiful-soup-web-scraper-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
